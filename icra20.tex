%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%2345678901234567890123456789012345678901234567890123456789012345678901234567890
%        1         2         3         4         5         6         7         8

\documentclass[letterpaper, 10 pt, conference]{ieeeconf}  % Comment this line out if you need a4paper


%\documentclass[a4paper, 10pt, conference]{ieeeconf}      % Use this line for a4 paper

\IEEEoverridecommandlockouts                              % This command is only needed if 
% you want to use the \thanks command

\overrideIEEEmargins

% Sorts and compresses references properly
% (poor substitute for natbib, but it's the best we can do with this docclass)
\usepackage{cite}

% Give us pretty subfigures
\usepackage{subfig}

% Good math support
\usepackage{mathtools}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{caption}
\DeclareCaptionFont{eightpt}{\fontsize{8pt}{9pt}\selectfont #1}
\captionsetup{font=eightpt}

\graphicspath{{figures/}}

% Needed to meet printer requirements.

% See the \addtolength command later in the file to balance the column lengths
% on the last page of the document

% The following packages can be found on http:\\www.ctan.org
%\usepackage{graphics} % for pdf, bitmapped graphics files
%\usepackage{epsfig} % for postscript graphics files
%\usepackage{mathptmx} % assumes new font selection scheme installed
%\usepackage{times} % assumes new font selection scheme installed
%\usepackage{amsmath} % assumes amsmath package installed
%\usepackage{amssymb}  % assumes amsmath package installed


\title{\LARGE \bf
	I Picture You: \\
	Imaginative localization on remote teammates
}
% Automated synthesis of scalable algorithms for inferring non-local
%properties to assist in multi-robot teaming


\author{Taeyeong Choi, Sehyeok Kang, and Theodore P.~Pavlic % <-this % stops a space
	\thanks{*This work was not supported by any organization}% <-this % stops a space
	\thanks{T.~Choi, S.~Kang, and T. P.~Pavlic are with the School of Computing, Informatics, and Decision Systems Engineering,
		Arizona State University, Tempe, AZ 85281, USA
		{\tt\small \{tchoi4, tpavlic, aricha\}@asu.edu}}%
}


\begin{document}
	
	
	
	\maketitle
	\thispagestyle{empty}
	\pagestyle{empty}
	
	
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\begin{abstract}
		
%		In robot teaming, a robot that is able to understand the formation of 
%		entire team in real time may perform adjustive actions for the sake of the team.
%		
%		comm free
%		
%		Individual robots in multi-robot teams usually behave according to
%		relatively simple and often deterministic rules. This behavioral
%		regularity may allow for variations in one robot's behavior to provide
%		useful information about the state of other robots in the team. Thus,
%		coordinated motion between distantly separated robots could be achieved
%		without direct communication. 

		We propose \emph{IPY}, designed to tackle the remote teammate localization problem
		introduced 
		by \cite{Choi17} where a robot embedded in a multi-robot team is to predict positions 
		of all other teammates in which each robot has a limited sensor radius and a 
		relatively simple motion rule with dependency on its nearest neighbors. 
		They presented a data-driven method to train the predicting robot only in 
		a 3-robot team and extend it to larger teams without re-training in 
		communication-free environment. 
		One of the challenges in scaling up the prediction ability was, however, that
		errors in prediction for nearer teammates highly deteriorated prediction accuracy 
		for farther ones, since the predictor took the outcome for the nearer as input to make
		predictions for the more distant. 
		To reduce such propagated errors, \emph{IPY} not only employs a state-of-the-art deep
		neural network but utilizes more ample representations of all observations and its 
		intermediate predictions, which are visualized onto two dimensional images to 
		present predicted probability distribution over possible coordinates instead of 
		giving a numerical coordinates only. 
		As opposed to the previous work in \cite{Choi17}, which relied on computer simulations, 
		we show our method on a real robotic platform, \emph{Thymio}, to demonstrate that 
		it can provide robust prediction power even when the size of robot team increases, and 
		the prediction outcome is more interpretable.  
		
%		similar data-driven method 
%		to obtain a robot that can utilize observations on its neighbor and 
%		infer positions of distant teammates. A distinct feature is, however, to employ 
%		visual images to represent all the intermediate information during 
%		prediction process. 
		
		
		
		
%		Unfortunately, the dynamics describing a
%		collective multi-robot system will often be too complex to analytically
%		derive the relationship between observed nearest-neighbor variations and
%		environmentally driven changes in the behavior of remote robots.
%		
%		Artificial neural networks~(ANNs) may be used to find this relationship
%		within training data, but scalability of the approach requires that the
%		resulting ANNs be functional even in teams with sizes not represented in
%		the training data. To this end, we train a communication-free,
%		localization ANN on one robot in a 3-robot team and show how it can be
%		extended without re-training to larger teams. We test our approach in a
%		distributed caging scenario where a chain of simulated robots searches
%		for an object to encircle and executes a coordinated behavior, ideally
%		with no communication, shortly after only one detects the object.
	\end{abstract}
	
	
	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	
	\section{Introduction}
	\label{sec:intro}
	
	In multi-robot systems including swarms, every robot is usually allowed to observe 
	only a subset of its team members and interact with them to take the next action 
	according to relatively simple motion rules. 
	Such a property enables the entire system to perform in a distributed manner, and 
	also the behavior of the team can be driven by some of robots embedded in it to 
	achieve the goal [x]. 
	This implies that if a robot has an ability to recognize useful property, e.g.) formation, 
	of the whole team in real time, which could be only observed in a global view,
	it could present adjustive actions to promote a better collective behavior 
	for the sake of the team.
	
	A few works such as \cite{Choi17} have suggested a data-driven approach by which a robot in 
	networked multi-robot system can learn behavioral regularity of its neighbors to 
	predict the position of all others without any explicit communication.
	They emphasized scalability of their approach accomplished by repetitive applications of a regression model, which has been trained in a modular robot team, to larger teams.  
	Such a design, however, causes errors from prediction in a modular team to amplify 
	those in the following modular. 
	
	To gain more robust performance, we present \emph{IPY} that uses imaginative representation 
	for all observations and its prediction outcomes, even though in \cite{Choi17}, 
	a regression model was to calculate one numerical estimate per target state variable. 
	The imaginative format allows to express discretized probability distribution 
	over possible states of predicted teammate so that \emph{IPY} can show not only its best
	prediction information but how confident it is on the outcome. Because the 
	visualized prediction in a modular robot team is used in the next modular team, 
	it could assist the next prediction as a more informative feature.  

	during predictions for following modular teams. More specifically, the probability 
	distribution of predicted position is projected onto a two dimensional vector as an image 
	visualization, while that of inferred orientation is on a dimension of one dimension. 
	
	\emph{IPY} also takes into account the historical state information known so far, 
	since 
	
	trains a deep neural network, which runs multiple convolutional layers and 
	recurrent layers as backbone, to handle 1) historical state information known so far and
	2) the imaginative   
	
	
	model advanced 
	consider errors in output and input
	
	Using a more powerful machine learning model could help obtain more robust performance
	to some extent. In addition to it, we propose \emph{IPY} that generates two dimensional 
	images to represent probability distribution for predicted states of teammates such as 
	position and orientation. 
	This enables the predictor to learn to show its predictive uncertainty on possible 
	states and also to synthesize previous uncertainties to make current prediction. 
	
	Our contributions are as follows: 
	\begin{itemize}
		\item historical 
		\item Application of deep neural network 
		\item Visual representation 
		\item Physical robot implementation 
	\end{itemize}
	
	%
	\begin{figure}\centering
		\subfloat[Multi-robot chain]{\label{fig:SampleHFT}%
			\includegraphics[width=0.49\columnwidth]{fig_HeadAndTail}}
		\subfloat[Caging scenario]{\label{fig:SampleCaging}%
			\includegraphics[width=0.49\columnwidth]{fig_SampleCaging}}
		\caption{Motivating examples.
			%Robots move in a chain formation until encountering an object to
			%be encircled.
			In \protect\subref{fig:SampleHFT}, heterogeneous robots move in
			chain formation from right to left. In
			\protect\subref{fig:SampleCaging}, the \emph{Tail} robot breaks
			out of chain formation to more rapidly encircle an object
			encountered first by \emph{Head}.
		}
		\label{fig:SampleChains}
	\end{figure}
	%
	
	
	In networked multi-robot teams, coordinated behaviors typically emerge
	from interactions among adjacent robots with limited sensor range,
	simple motion rules, and no persistent connection to any centralized
	command and control authority. In principle, such a decentralized
	approach naturally scales well to real-world problems where a large
	number of robots must be deployed over relatively large areas where
	constant global communication is impractical. Significant effort has
	been focused on the design of decentralized control algorithms for
	multi-robot systems~\cite{%
		WK02, %
		CM04, %
		OA10, %
		NK11, %
		%HBNB13, %
		BFBD13, %
		RCN14, %
		WPKBPB14, %
		DGRSST14, %
		YDJH15, %
		VHD15, %
		VH15, %
		EB16, %
		VFHD16%
	}. However,
	approaches that make use of high levels of communication among agents
	present scalability challenges, and approaches that eschew communication
	often achieve less than ideal coordination as a consequence.
	
	If a single robot could, with little explicit communication, gain
	awareness of the status of other very remote agents in the team or
	awareness of some macroscopic state of the group, then a new class of
	highly scalable coordinated behaviors would be possible. Robots in one
	location could perform behaviors that complement the actions of robots
	in another location in a way that is currently only possible using
	levels of signalling that are particularly burdensome in large-scale
	groups. It is now possible to shift from explicit to implicit
	communication, from what behavioral ecologists call ``signals'' to
	``cues''~\cite{BV11}, because of the recent increase in computational
	power available on modern small robotic platforms. Whereas the platforms
	of the past compensated for little local computational power with
	communication to coordinate with other agents, robotic platforms of the
	future can rely on high levels of local computational power to reduce
	the need for communication that may be energetically costly or
	physically infeasible.
	
	Toward this end, we propose a machine-learning approach for an
	individual robot in a multi-robot team to estimate the position of its
	team members using only local information about the movements of a
	single nearby robot. This approach capitalizes on the regularity of the
	simple behavioral rules used by members of the multi-robot team; that
	is, the variability in observed behaviors of nearby team members tends
	to be a strong predictor of changing environmental conditions elsewhere
	in the team because the robots operate in a very predictable way under
	nominal conditions.
	
	To demonstrate our approach in this paper, we consider a simple scenario
	where a heterogeneous multi-agent system like the one in
	Fig.~\ref{fig:SampleHFT} maintains a chain
	formation and consists of three types of nonholonomic mobile robots:
	\emph{Head}, \emph{Follower}, and \emph{Tail}.
	
	\emph{Head} is a robot at one end of the chain formation, \emph{Tail} is
	another at the distal end, and between them is a string of
	\emph{Follower} robots. Our goal is to train artificial neural
	networks~(ANNs)~\cite{Schmidhuber15} on \emph{Tail} to learn how to
	utilize locally observable information~-- the position of its nearest
	neighbor~-- to estimate the position of every robot in the system even
	including the \emph{Head} when it is outside of the sensor range of the
	\emph{Tail}. The major strength of our approach is that training of the
	ANNs on \emph{Tail} requires only a 3-robot chain, and the resulting ANN
	can be applied by \emph{Tail} to fix the positions of robots in longer
	chains simply by applying the ANN repeatedly in a recursive fashion. We
	validate our approach in a simulation of robots with realistic dynamics
	in the presence of noise and little-or-no communication availability. We
	show that the approach can facilitate coordinated behaviors like the
	one in Fig.~\ref{fig:SampleCaging}.
	
	% The major strengths of our approach are that it scales naturally with an
	% increase in the length of the robot chain, and it does not require
	% costly or potentially infeasible communication among agents. Just as
	% humans in groups can use prior knowledge of natural regularities in the
	% actions of those around them to infer the state of the group only by
	% observing the actions of a small subset of its members, the robots in
	% our proposed multi-robot teams capitalize on regularities in the other
	% robots around them to gain situational awareness of the group as a
	% whole. Furthermore, we use this encirclement example as a proof of
	% concept that machine-learning techniques can be used to achieve global
	% situational awareness in other applications where an autonomous agent is
	% embedded (perhaps surreptitiously) into a collective. Although we focus
	% on multi-robot systems here, a similar technique could also be used in
	% an Internet-of-Things~\cite{AIM10} scenario (for example) to predict
	% some emerging large-scale situational property of human or
	% mixed-autonomy groups (e.g., level of panic, onset of congestion in an
	% evacuation scenario, etc.) using only locally sourced measurements
	% around each sensor distributed throughout the group. Thus, our methods
	% are not only a mitigation strategy for lacking communication in
	% distributed autonomous systems but also a general tool for capitalizing
	% on intrinsic system regularities to better infer large-scale properties
	% of multi-agent systems based only on local properties.
	
	% To demonstrate the utility of this approach, we empirically evaluate the
	% performance in different scenarios. \emph{Tail} is to estimate the
	% position of its all teammates while the team is moving straight or
	% making a turn. The accuracy is evaluated for different numbers of robots
	% in the robotic team. We also explore the case where intermittent
	% communication is available to find if our methodology can interpolate
	% the infrequent pulses of reliable information. Next, we take advantage
	% of ANNs trained in the presence of noise to test reliability and
	% applicability to realistic problems where observable data can be
	% corrupted. As the last demonstration, we introduce a motivating scenario
	% (Fig.~\ref{fig:SampleCaging}) where the robot team must encircle a
	% target object that is initially detected by the \emph{Head} that will
	% lead the convoy around the object. While the team explores the
	% two-dimensional space to find the object, \emph{Tail} continues to
	% recognize the pose of every teammate. When \emph{Head} detects the
	% target object and begins to turning to cage it, \emph{Tail} recognizes
	% the signature of this remote behavior in its nearest neighbors and
	% executes a complementary maneuver so that its end of the chain moves
	% efficiently toward the final target formation around the encountered
	% object.
	
	This paper is organized as follows. In Section~\ref{sec:related_work},
	we review related work and our contribution beyond it. Next, we formally
	define our system and its motion rules in Section~\ref{sec:sysdesign}.
	Then, in Section~\ref{sec:estimation}, we describe our approach to
	training ANNs for the estimation tasks. Our approach to both estimation
	and the encirclement solution is validated in
	Section~\ref{sec:experiments} on a simulation framework developed for a
	relatively new multi-robot testbed, the
	\emph{Robotarium}~\cite{PWGDMAFE16}. Finally, we give concluding remarks
	in Section~\ref{sec:conclusion}.
	
	\section{Related Work}
	\label{sec:related_work}
	
	In the context of the example scenario presented in this work, our goal
	is to enable an individual robot, \emph{Tail}, to infer the positions of
	its teammates in real time within its frame of reference. This task is
	different from traditional localization problems where a robot fixes its
	position given its action and measurement models~\cite{Thrun05}.
	Moreover, our problem differs from scenarios with multiple robots
	cooperating to track a target's state in a distributed way~\cite{FSDO10,
		CX14, DMG15} because we impose a hard constraint on information sharing
	between robots, and all robots other than \emph{Tail} do not perform any
	localization inferences of the pose of others in the team.
	
	Our work is superficially similar to work by Novitzky et
	al.~\cite{NPCBW12} and Das et al.~\cite{DCV16} where robots visually
	signal to others like the ``waggle dance'' of
	honeybees~\cite{VonFrisch67, DC04}. Strictly speaking, these behavioral
	communication channels still represent explicit signalling between
	agents. In the study of animal communication, such signalling evolved
	when the benefit of the explicit message passing outweighed the
	implementation costs~\cite{BV11}. Thus, in robotic scenarios where
	explicit signalling is notably costly, it may be useful to train robots
	to be responsive to more subtle cues that covary with environmental
	information; in that perspective, we are not building a motion-based
	signalling system between robots but achieving useful inferences about
	the group based on behavioral cues and not costly, explicit
	communication.
	
	There are stronger similarities between our work and methods from
	human--robot interaction where robots learn to interpret and respond to
	\emph{cues} (as opposed to explicit \emph{signals}) from human
	teammates. For example, Maeda et al.~\cite{MELAPN14} designed a
	framework with which a robot can infer how to appropriately assist a human
	partner assembling an item after learning from data collected from two
	humans completing the task. Thus, the robot can infer the normally
	cryptic intention of the human through non-verbal cues and can then
	respond appropriately. Our approach is similar, but our agent learns how
	to respond appropriately to cues from a complex system of other robots.
	
	In our running example of a moving chain formation, the formation can be
	driven by two heterogeneous leaders, \emph{Head} and \emph{Tail}. A
	similarly structured robotic system was designed by Elamvazhuthi and
	Berman~\cite{EB16} consisting of a long string of holonomic point robots
	that are terminated by two \emph{leader} agents. While the interior
	robots in the chain perform simple, wave-like motion rules based on
	position information from neighbors only, the two leader robots have
	pre-programmed trajectories that are guaranteed to drive the interior
	robots to desired locations along a formation. In contrast, our goal is
	to infer remote properties of the multi-robot team based on local
	information even when the individual-robot motion models are too complex
	to be analyzed mathematically. Thus, our methods generalize to more
	realistic robotic motion rules~-- such as nonholonomic robots limited to
	two-dimensional planar motion with collision avoidance.
	
	% \paragraph{Heterogeneous Leadership} Several researchers have
	% investigated the impact of heterogeneous leadership in robotic swarms
	% that emerges out of simple collective motion rules. Modeling natural
	% systems, Couzin et al.~\cite{CKFL05} show that large groups of mobile
	% agents whose simple motion rules lead to flocking behavior can be
	% guided by a small set of informed agents whose more deliberate motion
	% gently drives the flock in one direction or another. Schultz et
	% al.~\cite{SPS08} show that honeybees flying in a swarm are likely
	% being guided by ``streaker bees'' whose excessively fast motion can
	% effectively steer in a swarm using a similar but qualitatively
	% different mechanism than proposed by Couzin et al. In both cases,
	% agents achieve coordination or consensus not through explicit message
	% passing but by observations of the actions of a few notably different
	% individuals. Inspired by this idea, Genter and Stone~\cite{GS16} and
	% their collaborators~\cite{GAS13, GZS15} recently developed
	% communication-free, multi-agent coordination techniques based on
	% heterogeneous leaders embedded within simple multi-robot flocking
	% models. In their design, a small set of informed robots called
	% \emph{influencing} agents were added to a larger set of flocking
	% agents that adjust their movement behavior based on the motion of
	% surrounding neighbors. The influencing agents use different motion
	% rules than the background flock, and consequently have the power to
	% exert disproportionate influence on the flock's center of mass~--
	% analogous to a group coming to consensus in the presence of a single
	% member whose opinion is significantly more difficult to change. A
	% similar, albeit not bio-inspired, approach was pursued by Elamvazhuthi
	% and Berman~\cite{EB16} for the case of a long string of robots in
	% chain formation that are terminated by two \emph{leader} agents. While
	% the interior robots in the chain are driven by simple, wave-like
	% motion rules, the two leader robots have pre-programmed trajectories
	% that are guaranteed to drive the interior robots to desired locations
	% along a formation~-- somewhat like a ``jump rope'' being driven by
	% controlled twirling at both ends. While we are motivated by the
	% application of informed robots (influencers or leaders) to improve the
	% efficiency of multi-robot systems, our primary goal is the on-line
	% inference of global or remote characteristics of a multi-robot team
	% based on local information. Moreover, our methods generalize to more
	% realistic robotic motion rules~-- such as nonholonomic robots limited
	% to two-dimensional planar motion with collision avoidance.
	
	% \paragraph{Multi-Robot Caging} Our motivating application example, the
	% problem of a team of robots designed to approach an object and
	% coordinate so as to encircle or ``cage'' the object, has also been
	% addressed before~\cite{WK02, FSDO10, YDJH15}. Wang and
	% Kumar~\cite{WK02} defined the notion of \emph{object closure}, which
	% characterizes a condition when a surrounding object has no feasible
	% path of escape beyond some threshold distance. They were able to
	% synthesize a decentralized control strategy for achieving object
	% closure so long as the team consists of identical, holonomic robots
	% that can each estimate geometric properties of the object. Since then,
	% Franchi et al.~\cite{FSDO10} have developed distributed strategies for
	% encircling an object with a team of robots whose motion is more
	% realistically constrained, and they implemented their approach on a
	% physical testbed of \emph{Khepera~III} robots surrounding a mobile
	% target. More recently, bio-inspired solutions for caging and boundary
	% coverage have also been proposed. For example: Yang et
	% al.~\cite{YDJH15} have developed a decentralized algorithm for caging
	% based on bacterial chemotaxis;
	
	Our motivating application example, caging, is a popular scenario in the
	robotics literature. Wilson et al.~\cite{WPKBPB14} developed a
	stochastic robotics method for boundary coverage meant to mimic the
	collective transport behavior of ants, and Derakhshandeh et
	al.~\cite{DGRSST14} developed an amoeba-inspired distributed algorithm
	for coating objects on a specially defined grid. Each of these
	applications makes use of identically controlled agents that come to
	equilibrium in some desired formation, often with the assumption of a
	simplistic motion model. In contrast, we focus on heterogeneous
	multi-agent systems where some agents must gain global situational
	awareness in order to make decisions about whether and when to switch
	from certain behavioral modes to certain other modes. In principle, our
	method could be used in combination with these other techniques so that
	agents can determine when to switch to other behaviors after the desired
	configuration has been reached.
	
	% \paragraph{Supervised Learning and Neural Networks} To develop our
	% inference algorithm on our \emph{Tail} robot, we make use of supervised
	% machine learning to train artificial neural networks. The automated
	% synthesis of controllers for multi-robot teams has been proposed in the
	% past. As reviewed by Brambilla et al.~\cite{BFBD13}, most approaches
	% borrow methods from evolutionary robotics~\cite{NF00} to tune parameters
	% of individual controllers to maximize some performance metric at the
	% level of the group. In particular, genetic algorithms are commonly
	% applied to the design of weights for artificial neural networks that map
	% robot inputs to outputs. While this approach is very powerful in
	% principle, it has been criticized for producing solutions that lack
	% generality~\cite{TN11}. Whereas some authors suggest that the
	% flexibility of neural networks may actually be a weakness and suggest
	% more constrained frameworks instead~\cite{MHA13, FBBTB14}, others have
	% been successful automatically synthesizing multi-robot control policies
	% by enriching neural networks with even more flexibility~\cite{TD05,
	% TSEHBRD08}. Although we also use a supervised learning approach similar
	% to these earlier results to tune the weights of an artificial neural
	% network on an individual robot in a multi-robot team, we are not
	% attempting to develop motion rules shaped by selective pressures at the
	% level of the team. Instead, we develop an inference engine on a single
	% robot that can be used in lieu of communication or conventional
	% localization. Thus, whereas others use neural networks as a rich space
	% through which to search for efficient motion controllers in multi-robot
	% teams, we use neural networks as statistical models for the complex
	% dynamics of other robots in the team.
	
	\section{System Design}
	\label{sec:sysdesign}
	
	We consider a multi-agent system consisting of $n$ nonholonomic mobile
	robots that move within a chain formation in the plane of
	$\mathbb{R}^2$. At all times, each one has a pose denoted by a three
	dimensional vector $(x, y, \theta)$ where $x$ and $y$ specify its
	position and $\theta$ its orientation. Every robot is controlled by a
	decentralized controller and can only detect the distance and angle to
	another robot or other object within a limited sensor radius. Due to
	this limitation, each robot cannot localize in the global coordinate
	system but can use their own frame of reference to represent observed
	objects. The frame of reference will be explained in detail in
	Section~\ref{sec:estimation}.
	
	Each robot type from Fig.~\ref{fig:SampleHFT} has a distinct motion
	rule.
	%
	\begin{itemize}
		\item The \emph{Head} moves independently in the given space.
		\item The $n-2$ \emph{Follower} robots continuously update their
		position to maintain a constant distance between their two
		nearest neighbors.
		\item The \emph{Tail}
		either follows its single neighbor at a constant distance or
		moves independently.
	\end{itemize}
	%
	So as the \emph{Head} moves, it pulls a nominally rigid chain of robots
	behind it, but \emph{Tail} can switch to moving independently of the
	others, causing the chain to be pulled at both ends.
	
	Formally, we define the set of robots $\mathcal{R} \triangleq \{1, 2,
	\ldots, n\}$ where robot~$n$ is the \emph{Head}, robot~$1$ is the
	\emph{Tail}, and robot~$i \in \{2, \ldots, n-1\}$ is a \emph{Follower}.
	Every robot $i \in \mathcal{R}$ is assumed to abide by nonholonomic
	unicycle kinematics of the form
	%
	\begin{equation*}
		%    \left\{
		%        \begin{aligned}
		%            \dot{x}_i &= v_i \sin(\theta_i)\\
		%            \dot{y}_i &= v_i \cos(\theta_i)\\
		%            \dot{\theta}_i &= \omega_i
		%        \end{aligned}
		%    \right.
		\dot{x}_i = v_i \sin(\theta_i),
		\qquad
		\dot{y}_i = v_i \cos(\theta_i),
		\qquad
		\dot{\theta}_i = \omega_i
	\end{equation*}
	%
	where vector $\vec{p}_i \triangleq (x_i, y_i)$ is the robot's Cartesian
	position, $\theta_i$ is the robot's orientation, $v_i$ is the linear
	velocity of the robot, and $\omega_i$ is the angular velocity of the
	robot. The \emph{pose} $\vec{r}_i \triangleq (x_i, y_i, \theta_i)$ of a
	robot~$i \in \mathcal{R}$ is a vector containing its position in the
	plane and its heading orientation. Thus, by adjusting $\omega_i$ and
	$v_i$, the nonholonomic robot can be driven like a unicycle over the
	plane. We use this model for simplicity. However, the data-oriented
	machine learning approach described in Section~\ref{sec:estimation}
	should be applicable to a wide range of other kinematic and dynamic
	models as well.
	
	To implement the chain-following formation shown in
	Fig.~\ref{fig:SampleHFT}, we use fully actuated, holonomic kinematics
	in the Cartesian plane to generate \emph{reference trajectories} for
	$\omega_i$ and $v_i$ to track within the more realistic nonholonomic
	unicycle kinematics. As described above, the underlying robots
	attempting to track these reference behaviors will do so with some error
	because they are nonholonomic. So the actual behavioral rules are a
	composition of simplistic holonomic kinematics with the more realistic
	nonholonomic kinematics of the simulated robots. The particular
	coordinate transformation and nonholonomic constraints are implemented
	within the controller in the simulator for the multi-robot testbed that
	we use to train and test the system (see Section~\ref{sec:experiments}),
	and so we only present the fully actuated, target motion rules here. In
	particular:
	%
	\begin{itemize}
		\item The \emph{Head} robot approximates the motion rule:
		%
		\begin{equation*}
			\dot{\vec{p}}_n = \vec{T}_h - \vec{p}_n
		\end{equation*}
		%
		where $\vec{T}_h$ is some target Cartesian position in the space.
		Thus, the \emph{Head} robot is always moving toward some target
		point set elsewhere in its control hierarchy.
		
		\item Each \emph{Follower} robot~$i \in \{2,\ldots,n-1\}$
		approximates:
		%
		\begin{multline*}
			\dot{\vec{p}}_i
			=
			k_f(
			(\|\vec{p}_i - \vec{p}_{i-1}\| - d)
			(\vec{p}_{i-1} - \vec{p}_i)\\
			+
			(\|\vec{p}_i - \vec{p}_{i+1}\| - d)
			(\vec{p}_{i+1} - \vec{p}_i)
			)
		\end{multline*}
		%
		where $k_f$ is a scalar that scales the difference between the
		desired spacing around robot~$i$ to a desired velocity that
		should, if geometrically possible, return the \emph{Follower} to
		a distance of $d$ away from its two neighbors.
		
		\item The \emph{Follower} normally behaves like a \emph{Follower}
		with one neighbor, as in:
		%
		\begin{equation*}
			\dot{\vec{p}}_1
			=
			k_t(
			(\|\vec{p}_1 - \vec{p}_{2}\| - d)(\vec{p}_{2} - \vec{p}_1)
			).
		\end{equation*}
		%
		However, it will switch to the \emph{Head} motion rule when it
		detects that \emph{Head} has detected some object to encircle.
		
	\end{itemize}
	%
	Thus, the challenge to be addressed in Section~\ref{sec:estimation} is
	how the \emph{Tail} can use regularities in the patterns of interactions
	that emerge from such simple motion rules to predict the position of
	\emph{Head} using only knowledge about the position of robot~2, the
	\emph{Follower} closest to \emph{Tail}.
	
	\section{Estimation}
	\label{sec:estimation}
	
	Our goal is to design an essentially communication-free algorithm that
	enables \emph{Tail} to estimate the position of each distant robot~$i
	\in \{3,\dots,n\}$ in the team using only locally sensed information
	about robot~$2$, the \emph{Follower} immediately ahead of it.
	Furthermore, the same algorithm should be able to be used for a wide
	range of team sizes. The scalability of our approach comes from our use
	of modular artificial neural networks that are applied recursively as
	opposed to a monolithic ANN tailored for a particular team size.
	
	\subsection{Supervised Learning of Artificial Neural Networks}
	\label{sec:supervisedlearning}
	
	Toward accomplishing our goal, we first consider the simple 3-robot case
	($n=3$) where robot~3 is the \emph{Head}. On \emph{Tail}, we use
	backpropagation~\cite{B06} to train a set of ANNs, $\{ANN_{x}, ANN_{y},
	ANN_{\theta}\}$, as regression models that can predict the Cartesian
	coordinates of robot~$3$ as well as the orientation of robot~$2$ using
	only the sensed coordinates of robot~$2$. They are trained with pose
	data collected at discrete time instants during interactions among the
	three robots in simulations where robot~$3$ takes arbitrary motions.
	
	From this point forward, we make use of subscripts form $i@\tau\to j@t$
	to denote that a coordinate element of robot~$i$ at time~$\tau$ is
	represented in the local coordinate frame of robot~$j$ at time~$t$. Each
	ANN is a multi-layer perceptron employing three layers~-- one input
	layer, one hidden layer, and one output layer. The hidden layer utilizes
	12 nodes, each of which performs computations that take values from all
	nodes of the input. Each hidden node uses the logistic, sigmoidal
	activation function whose output ranges continuously from 0 to 1. For
	regression purposes, one node set in the output layer has a linear
	activation function taking outputs of all hidden nodes as input. The
	major difference in the structure of the three ANNs is in their input
	layer. The $ANN_{x}$, which is used to predict $\vec{x}_{3@t\to 1@t}$,
	takes six inputs consisting of:
	%
	\begin{itemize}
		\item $\vec{r}_{2@t\to 1@t}$, the relative pose of robot~$2$ in frame
		of robot~$1$ at time~$t$
		
		\item $\vec{p}_{2@t+1\to 1@t}$, the relative coordinates of robot~$2$
		at time~$t+1$ in frame of robot~$1$ at time~$t$
		
		\item $b$, a scalar bias
	\end{itemize}
	%
	In training, the $ANN_{y}$ that estimates $\vec{y}_{3@t\to 1@t}$ uses
	these same six inputs plus $\vec{x}_{3@t\to 1@t}$. Similarly, the
	$ANN_{\theta}$ for $\vec{\theta}_{2@t+1\to 1@t}$ takes the seven inputs
	of $ANN_{y}$ as well as $\vec{y}_{3@t\to 1@t}$. For the $n > 3$ case, we
	explain how coordinate transformations allow these ANNs to be applied
	recursively to predict all robot positions in
	Section~\ref{sec:scalableestimation}.
	
	\subsection{Estimation Process}
	\label{sec:estimationprocess}
	
	% $\vec{r}_{1@t\to 1@t-1}$ 
	For estimation with $n=3$, the ANNs on \emph{Tail} can estimate
	$\vec{p}_{3@t-1\to 1@t-1}$ and $\theta_{2@t\to 1@t}$ at any time $t$
	using only the locally sensed position of robot~2 (assuming the initial
	orientation of all robots are known). Throughout the estimation process,
	the $\vec{x}_{3@t\to 1@t}$ input to $ANN_{y}$ and $ANN_{\theta}$ will be
	estimated by $ANN_{x}$, and the $\vec{y}_{3@t\to 1@t}$ input to
	$ANN_{\theta}$ will be by $ANN_{y}$. For example, at the second time
	instant $t=1$, the \emph{Tail} starts to estimate the position
	information of robot~$3$ for time~$0$ and thus also the orientation of
	robot~$2$ for time~$1$. In other words, by utilizing the sequential
	observations of $\vec{p}_{2@0\to 1@0}$ and $\vec{p}_{2@1\to 1@0}$ and
	the already known initial orientation, \emph{Tail} can estimate
	$\vec{p}_{3@0\to 1@0}$ as well as $\theta_{2@1\to 1@0}$, which can be
	transformed to $\theta_{2@1\to 1@1}$ for future computation. Similarly,
	for every time instant~$t$, \emph{Tail} can estimate $\vec{p}_{3@t-1\to
		1@t-1}$ and $\theta_{2@t\to 1@t}$.
	
	\subsection{Scalable Estimation}
	\label{sec:scalableestimation}
	
	As described in Section~\ref{sec:supervisedlearning}, our approach
	trains three ANNs for the $n=3$ case and applies them recursively to
	estimate the positions of robots in the $n>3$ case. An alternative
	approach would be to apply ANNs trained on data for cases where
	$n\geq 3$. However, training under that approach would be complicated by a
	relatively large state space, and a new training phase would be required
	whenever the number of deployed robots changed.
	
	For scalability and to simplify the training process, we take advantage
	of a coordinate transformation. Assume that $n>3$ and \emph{Tail} has
	processed the estimation until $t=2$ as described above. \emph{Tail} can
	transform the estimates for robots~2 and~3 into the reference frame of
	robot~2 to predict $\vec{p}_{4@0\to 2@0}$; that is, the \emph{Tail} can
	act as if it is robot~2 predicting the position of robot~4. The
	predicted position also can be transformed back to the natural reference
	frame of \emph{Tail} for it to gain the global awareness. By such a
	repeated computation, \emph{Tail} can finally obtain estimates for the
	full pose information up to robot~$i \in \{3,\ldots,n-1\}$ and thus also
	the position information up to robot~$(i+1)$ in finite time. Estimates
	of a far robot~$i \in \{3,\ldots,n\}$ will be delayed, but this delay
	scales only linearly with the distance from the \emph{Tail}, which is no
	worse than would be expected with direct communication of position in a
	distributed, ad~hoc wireless network. Thus, the ANNs trained in the
	3-robot case are a general purpose tool for inferring pose information
	for arbitrary length chains.
	
	\section{Experiments}
	\label{sec:experiments}
	
	Experiments were conducted in the simulation platform designed for use
	with the \emph{Robotarium}~\cite{PWGDMAFE16}, a remotely accessible
	swarm-robotics testbed consisting of large numbers of \emph{GRITSBot}
	robots~\cite{PLE15}. The \emph{GRITSBot} is a nonholonomic, differential
	drive robot that is modeled explicitly within the Robotarium simulator,
	available in both \textsc{Matlab} and Python. In principle, robotic
	implementations tested within the simulator can easily be ported to the
	actual \emph{Robotarium} multi-robot testbed environment and executed
	remotely. For our simulated case, the rectangle-shaped arena has width
	1.2 and length 0.7, and the diameter and sensor radius of each robot
	are 0.03 and 0.12, respectively.
	
	\subsection{Training Procedure}
	\label{sec:learningphase}
	
	In the learning phase, only three robots, one \emph{Head}, one
	\emph{Follower}, and one \emph{Tail}, are involved as explained in
	Section~\ref{sec:supervisedlearning}. They are allowed to interact with
	each other according to the built-in motion rules described in
	Section~\ref{sec:sysdesign}. During the interactions, all pose
	information is recorded every time step to later train the three ANNs on
	\emph{Tail}. For efficient learning, \emph{Head} is designed to choose
	its actions from a predetermined set of linear and angular velocity
	values, leading the team to move forward or make different curves.
	
	In addition to the velocity of \emph{Head}, it is important to vary the motion-rule parameter
	$k_{t}$ on \emph{Tail} as well. The ANN is trained in a 3-robot scenario
	where the motion of \emph{Tail} is not constrained by any robots that
	follow its own motion; however, when applying the ANN recursively in the
	$n>3$ case as described in Section~\ref{sec:scalableestimation}, the
	fictitious \emph{Tail} robots in each recursion step will be more
	constrained than a true \emph{Tail}. By training with different values
	of $k_{t}$, the ANN is able to anticipate this reduced flexibility.
	
	Our training data is generated from the 20 combinations of the five
	\emph{Head} velocities with four $k_{t}$ parameter values. Each of the
	20 treatments was allowed to run for 800 time steps, and the resulting
	trajectories were replicated five times so that Gaussian noise $\epsilon
	\sim \mathcal{N}(0,0.001^{2})$ could be added to each datum to ensure
	stable performance of the ANNs even with noise. All ANNs were
	trained for 500 epochs with a learning rate of $0.05$ and momentum
	$0.2$.
	
	%Table~\ref{ann_result} shows the performance of ANNs that are trained by 5
	%cross validations by measuring Mean Absolute Error (MAE) and Root Mean
	%Squared Error (RMSE). The error for $x$ is relatively small compared to
	%$y$ and $\theta$ because we used a chain formation for the group of
	%robots and had learnings with curves in \emph{Tail}'s frame.
	%
	%\begin{table}[b]
	%\centering
	%\caption{Performance of trained ANNs}
	%\label{ann_result}
	%\begin{tabular}{p{1.5cm}p{1.5cm}p{1.5cm}p{1.5cm}}\hline 
	%& \emph{x} & \emph{y} & $\theta$ \\ \hline
	%MAE & 0.0009 & 0.0023 & 0.0022 \\ \hline
	%RMSE & 0.0014 & 0.003 & 0.0142 \\
	%\hline\end{tabular}
	%\end{table}
	
	% \begin{figure*}
	% \centering
	% \includegraphics[width=1\textwidth]{fig_straight_curve}
	% \caption{Estimation error while \emph{Head} is leading the other $n-1$ 
	%         robots in straight line (a) or curve (b,c). }
	% \label{fig:straight_curve}
	% \end{figure*}
	
	\subsection{Estimation Phase}
	\label{sec:estimationphase}
	
	To examine the feasibility and accuracy of our estimation method in various
	settings, we first show position estimation results from experiments
	where the \emph{Tail} only has access to local information about its
	immediate neighbor. Next, a different scenario is introduced where the
	ANN is used to interpolate between slow updates from periodic
	communication of other robot positions. Also, we test the effect of
	noise on the estimation performance.
	
	As a measure of estimation accuracy, the \emph{Euclidean distance}
	between the actual position of robot~$i \in \{3, ..., n\}$ and the
	corresponding estimate in \emph{Tail}'s perspective is calculated every
	time step. Estimates with an error below the robot diameter of $0.03$
	are considered to be accurate. Moreover, the testing duration $1,600$ is
	twice the training duration, which ensures that the ANN is tested with
	novel challenges.
	
	To differentiate between followers during the experiments, we use the
	naming convention that \emph{Follower 1} is adjacent to \emph{Tail},
	\emph{Follower 2} is adjacent to \emph{Follower 1}, \emph{Follower 3} is
	adjacent to \emph{Follower 2}, and so on. In other words, \emph{Follower
		$k$} refers to the follower that is $k$ robots ahead of \emph{Tail}.
	
	\subsubsection{Estimation without Communication}
	\label{sec:estwithoutcomm}
	
	Figure~\ref{fig:plots}a plots the accuracy in the simplest
	case where the \emph{Head} leads the other five robots to travel straight.
	%
%	\begin{figure}
%		\centering
%		\includegraphics[width=0.5\textwidth]{fig_plots}
%		\caption{Estimation errors. (a), (b), and (c) are cases when \emph{Tail}
%			performs the estimation without any assistance of communication.
%			Cases~(d) and~(f) allow \emph{Tail} to access to the actual
%			pose information of the team every $150$ time steps. (e) is
%			a noise-corrupted configuration of (b), and (f) is of (d).
%			%         and (b) and (c) involve Gaussian noise
%			%         $\epsilon \sim \mathcal{N}(0, 0.001^{2})$
%		}
%		\label{fig:plots}
%	\end{figure}
	%
	The estimation error is larger for farther robots as they inherit some
	error from estimations of nearer robots. Yet, every error is below
	$0.03$ and thus within our accuracy tolerance through $1,600$ steps.
	
	We also tested our model with a curve formation that is more complex
	than a straight line. Figure~\ref{fig:plots}b shows the performance for
	the case where the \emph{Head} leads a team of five robots to make a
	curve. The overall error is larger than the previous case, but every
	error remained in the acceptable level. The error peaked near $400$ time
	steps when the curve's angle was at the maximum during the execution.
	However, the error decreased after that peak as robots recovered from
	overshooting the turn and returned to their straight trajectory.
	
	Then, we explored a case of six robots, one more than the previous
	case, while making a turn. As shown in Fig.~\ref{fig:plots}c, the
	overall performance was worse than the two previous cases, as the
	estimate for \emph{Head} was inaccurate between $350$ and $750$ steps.
	Though the estimation for the other robots (\emph{Follower 2}, \emph{3},
	and \emph{4}) was accurate, the accumulated error caused unacceptable
	estimation error for \emph{Head}. Nevertheless, the performance pattern
	looks similar as the previous experiment in that the peak error occurs
	near $500$ steps and declines to below the $0.03$ accuracy threshold as
	the robots return to a more predictable straight-line trajectory after
	$750$ time steps.
	
	\subsubsection{Intermittent Communication}
	\label{sec:intermittentcomm}
	
	Although we have focused on scenarios where there is no robot-to-robot
	communication, we also tested the effect of intermittent communication
	providing limited information at a relatively slow rate. In reality, we
	could expect that robots can, in principle, broadcast their own pose
	data but do not frequently do so due to technological limitations or to
	save on communication cost. For instance, an aerial robot may only be
	able to share its pose periodically with a single base station that can
	relay it later to other robots that periodically fly by.
	
	In our case, every $150$ time steps, \emph{Tail} is allowed to access to
	actual position and orientation information for estimation process.
	Between messages, \emph{Tail} must use the ANN to infer the pose of
	others, but actual pose is known intermittently.
	
	Figure~\ref{fig:plots}d illustrates the improvement by the periodic
	communication, when six robots are turning a curve as previous setting.
	Just after the communication update, the error rapidly decreases toward
	zero due to the introduced information from the communication burst. These
	periodic decreases in error allow for longer periods of time when the total
	error is kept under the accuracy threshold.
	
	\subsubsection{Reliability Demonstration}
	\label{sec:reliabilitydemo}
	
	% \begin{figure*}
	% \centering
	% \includegraphics[width=1\textwidth]{fig_comm_noise}
	% \caption{Estimation error while \emph{Head} is leading the other $n-1$ 
	%         robots in curve. (a) and (b) allows the periodic communication,
	%         and (b) and (c) involve Gaussian noise 
	%         $\epsilon \sim \mathcal{N}(0, 0.001^{2})$ }.
	% \label{fig:comm_noise}
	% \end{figure*}
	
	% \begin{figure}
	% \centering
	% \includegraphics[width=0.5\textwidth]{fig_noise}
	% \caption{Estimation error while \emph{Head} is leading the other $n-1$ 
	%         robots in curve. (a) and (b) allows the periodic communication,
	%         and (b) and (c) involve Gaussian noise 
	%         $\epsilon \sim \mathcal{N}(0, 0.001^{2})$ }.
	% \label{fig:noise}
	% \end{figure}
	
	In this set of experiments, we tested the case where measurements were
	corrupted with noise. Specifically, we added noise $\epsilon \sim
	\mathcal{N}(0, 0.001^{2})$ to every data instance that is fed to the
	ANNs in order to examine if the scenarios that already showed successful
	results would lead to the similar performance in this situation as well.
	The distribution of noise was chosen to simulate that $99\%$ of
	measurements would have an error off the true value by $\leq 0.003$,
	$10\%$ of the diameter of simulated robots.
	
	Figure~\ref{fig:plots}e shows a noise-corrupted version of the
	scenario evaluated earlier in Fig.~\ref{fig:plots}b. Despite the
	additive noise, the ANNs still perform reliably well; every estimate is
	within the accuracy threshold over time. On average, the error
	has a similar shape to Fig.~\ref{fig:plots}b.
	
	The case in Fig.~\ref{fig:plots}f was configured as in the periodic
	communication case of Fig.~\ref{fig:plots}d with additive noise. The
	noise-corrupted results vary only slightly from the noise-free results,
	and so our approach can perform well even in a noisy environment.
	
	\subsubsection{Application Scenario}
	\label{sec:appscenario}
	
	Finally, we return to the motivating multi-robot team application that
	we introduced in Fig.~\ref{fig:SampleCaging}. The multi-robot team is
	to surround an object after being detected by \emph{Head}. Empirically,
	we found that caging cannot be accomplished in a team of only
	\emph{Head} and \emph{Follower} robots because the \emph{Follower}
	robots have no attraction to the object and thus will not circle it
	under their simple motion rules. By estimating the behavior of
	\emph{Head}, the \emph{Tail} can determine when the \emph{Head} has
	detected an object and roughly where the object is. At that point, the
	\emph{Tail} can move in a direction that ensures that the
	\emph{Follower} robots (which are influenced by both their forward and
	rearward neighbors) are pulled toward the object.
	
	In this scenario, we assume that \emph{Head} and \emph{Tail} can follow
	along with the surface of an object that they locally encounter in the
	environment. We also assume that \emph{Follower} uses a
	potential-field-based controller~\cite{Arkin98} to avoid collisions when
	in close proximity to the object. Because \emph{Head} will detect the
	object before \emph{Tail}, we will study the case where \emph{Tail}
	detects that \emph{Head} has begun a caging maneuver that can be
	assisted by \emph{Tail}. In that condition, \emph{Tail} will begin to
	encircle the object in the reverse direction of \emph{Head}. In the end,
	when \emph{Head} and \emph{Tail} are facing each other, they will switch
	to a \emph{Follower} motion rule so that the caging formation converges
	to equilibrium ``net'' around the object, in which all the robots
	maintain the same distance to neighbors over time.
	
	Figure~\ref{fig:caging} shows our implementation for~6 robots deployed
	for the caging mission on the simulator.
	%
	\begin{figure}\centering
		\includegraphics[width=0.5\textwidth]{fig_caging}
		\caption{Applicable caging scenario for a team with a \emph{Tail}
			that can infer changes in the behavior of the \emph{Head} from
			remote.%
			%(a) a 6-robot team searches the
			%area to discover and cage an object. (b) \emph{Tail} that can
			%estimate position of other robots expedite an assistive action,
			%when it detects \emph{Head} has begun caging the object. (c)
			%\emph{Head} and \emph{Tail} continues caging until they sense
			%each other. (d) The team achieves the goal converging to a
			%caging formation in the end.%
		}
		\label{fig:caging}
	\end{figure}
	%
	We placed a virtual, circular object for the mission at the center with
	a circular boundary drawn around it indicating the desired final
	positions of the robots. The robot team starts searching from bottom
	left as shown in Fig.~\ref{fig:caging}a. In Fig.~\ref{fig:caging}b,
	\emph{Head} has sensed the object and begins to move around it, and this
	motion is detected by \emph{Tail} that starts to deviate from simply
	following its nearest neighbor. Figure~\ref{fig:caging}c illustrates how
	the complementary actions of \emph{Head} and \emph{Tail} lead to an
	encircling maneuver, and Fig.~\ref{fig:caging}d shows the final
	formation.
	
	\section{Summary, Conclusion, and Future Work}
	\label{sec:conclusion}
	
	We have presented a scalable algorithm combining a machine learning
	approach with switching frames of reference, which enables an individual
	robot in multi-robot system to predict the position of unobservable
	teammates from the motions of an observable neighbor. We used a
	realistic robotic simulator with noise and nonholonomic kinematics to
	show that a robot at one end of a chain could successfully estimate the
	position of all other robots with very little explicit message passing.
	Although performance of the estimator was strongest when the formation
	took on simple shapes, like a line, the method was still able to
	estimate robot positions for more complex maneuvers. We showed that the
	method could allow robots to coordinate to encircle an encountered
	object and that the far-robot position inferences were sufficient for
	the robots to coordinate their actions.
	
	In future work, we plan to improve the overall performance of the
	estimation to better demonstrate the scalability of this general method
	to larger multi-robot systems and swarms. Also, we will consider more
	general multi-agent system applications~-- including situational
	awareness within Internet of Things~(IoT) devices and social media~-- in
	which local decision making could be improved by inferring global
	properties only from information gathered locally.

{\small
	\bibliographystyle{IEEEtran}
	\bibliography{IEEEabrv, IEEEexample}
}


\end{document}
