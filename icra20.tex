%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%2345678901234567890123456789012345678901234567890123456789012345678901234567890
%        1         2         3         4         5         6         7         8

\documentclass[letterpaper, 10 pt, conference]{ieeeconf}  % Comment this line out if you need a4paper


%\documentclass[a4paper, 10pt, conference]{ieeeconf}      % Use this line for a4 paper

\IEEEoverridecommandlockouts                              % This command is only needed if
% you want to use the \thanks command

\overrideIEEEmargins

% Sorts and compresses references properly
% (poor substitute for natbib, but it's the best we can do with this docclass)
\usepackage{cite}

% Give us pretty subfigures
\usepackage{subfig}

% Good math support
\usepackage{mathtools}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{amsmath}
\DeclareCaptionFont{eightpt}{\fontsize{8pt}{9pt}\selectfont #1}
\captionsetup{font=eightpt}

\graphicspath{{figures/}}

\title{\LARGE \bf
	Learning local behavioral sequences
	to better infer non-local properties in real multi-robot systems
}
% Automated synthesis of scalable algorithms for inferring non-local
%properties to assist in multi-robot teaming

\author{Taeyeong Choi, Sehyeok Kang, and Theodore P.~Pavlic, \IEEEmembership{Member, IEEE} % <-this % stops a space
    \thanks{*This work was supported in part by NSF grant \#1735579.}% <-this % stops a space
    \thanks{All authors are with Arizona State University, Tempe, AZ,
        85281, USA. T.~Choi, S.~Kang, and T.~P.~Pavlic are with the
        School of Computing, Informatics, and Decision Systems
        Engineering. T.~P.~Pavlic is also with the School of
        Sustainability and the School of Life Sciences.
        {\tt\small \{tchoi4, skang66, tpavlic\}@asu.edu}}%
}

\begin{document}

	\maketitle
	\thispagestyle{empty}
	\pagestyle{empty}


	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
	\begin{abstract}
        When members of a multi-robot team follow regular motion
        rules sensitive to robots and other environmental factors
        within sensing range, the team itself may become an
        informational fabric for gaining situational awareness without
        explicit signalling among robots. In our previous
        work~\cite{CPR17}, we used machine learning
        to develop a scalable module, trained only on data from
        3-robot teams, that could predict the positions of all
        robots in larger multi-robot teams based only on observations of the
        movement of a robot's nearest neighbor. Not only was this
        approach scalable from 3-to-many robots, but it did not require
        knowledge of the control laws of the robots under observation,
        as would a traditional observer-based approach.
        However, performance was only tested in
        simulation and could only be a substitute for explicit
        communication for short periods of time or in cases of very low
        sensing noise. In this work, we apply more sophisticated machine
        learning methods to data from a physically realized robotic team to
        develop Remote Teammate Localization~(RTL) modules that can be
        used in realistic environments. To be specific, we adopt
        Long--Short-Term--Memory~(LSTM)~\cite{HS97} to learn the
        evolution of behaviors in a modular team, which has the effect
        of greatly reducing errors from regression outcomes. In contrast
        with our previous work in simulation, all of the experiments
        conducted in this work were conducted on the \emph{Thymio}
        physical, two-wheeled robotic platform.
        %As an extension of our previous work~\cite{CPR17}, we
		%We propose a more powerful machine learning approach, as an extension of~\cite{CPR17},
		%to tackle the Remote Teammate
		%Localization~(RTL) problem where a robot member in a multi-robot team is to predict positions
		%of all other teammates only using the observations on its nearest neighbor without any
		%communication between robots.
		%%        In the previous work, we followed a realistic configuration
		%%        in which each robot had a limited sensor radius
		%%        As each robot had a relatively simple motion rule with dependency on its nearest neighbors,
		%In the previous work, we showed feasibility of a scalable method by which
		%a predictor robot was trained in a modular 3-robot team but could extend the prediction
		%to a larger
		%team without additional training, also suggesting an application of such an inference in
		%caging mission.
		%In this work, however, we focus mainly on 1) achieving better performance
		%to improve the applicability and 2) conducting evaluation in
		%more realistic environments. To be specific, we adopt a Long-Short Term Memory~(LSTM)~\cite{HS97}
		%to learn possible evolution of behaviors in a modular team helping reduce the errors
		%from regression outcomes. Furthermore, while the previous work relied only on computer simulations,
		%all the experiments here are conducted on a physical two-wheeled robotic platform, \emph{Thymio},
		%to demonstrate the performance gain under realistic constraints.
	\end{abstract}


	%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

	\section{Introduction}
	\label{sec:intro}

    In multi-robot systems including swarms, each robot typically has
    the capability to observe only a subset of its team members to
    determine its next action according to relatively simple motion
    rules. Consequently, these multi-robot systems are fundamentally
    distributed, having long-term outcomes that are all coupled together
    but with no simple whole-team coordination mechanism that can be
    taken for granted. One approach to facilitating coordination in such
    distributed systems is to establish a distinguished leader with a
    singular influence on all team members, e.g., \cite{EB16, DGRSS17,
    Stern18}. However, if others in the team in potentially influential
    positions could indirectly infer the state and intentions of the
    leader, complementary actions could extend the leader's direct
    influence and improve the performance of the collective. For
    example, recognition by one robot at the end of a formation of a
    non-trivial and informative formation deviation by a robot at the
    other end could trigger motions that allow \emph{both} robots to
    work together to appropriately move the collective~\cite{CPR17}.

    In our previous work~\cite{CPR17}, we used a machine-learning method
    to solve the RTL problem where a robot~(\emph{Tail}) at one end of a
    line formation of a multi-robot team is to predict positions of all
    other teammates only using local observations about a single nearby
    teammate. Because each robot has a limited sensor radius and a
    relatively simple motion rule that depends on the position of its
    nearest neighbors, the \emph{Tail} has to be able to learn the
    regularity of the observed motions of its neighbor to finally infer
    the poses of all other robots. We introduced a repetitive prediction
    scheme to use predictions about nearer teammates to make predictions
    on farther ones until the prediction reached the \emph{Head} robot
    at the other end in line formation. In a multi-robot simulation, we
    showed the feasibility of using the method in an example caging
    scenario in which the \emph{Tail} could recognize the early stages
    of a caging action of \emph{Head} and promote a proactive maneuver
    to better assist in coordinating the team to quickly enclose an
    encountered object in the environment.

	Figure~\ref{fig:Concept} illustrates our proposed pipeline in which
	a deep neural network is to learn to synthesize the observations of its neighbor
	with knowledge about historical positions of all the teammates.
	As shown in Fig.~\ref{fig:DL_Pipeline}, an LSTM layer is deployed to encode
    the historical sequence input, which could learn robot dynamics and
    the probable evolution of the team shape over time under physical
    constraints. Furthermore, the learned sequence encoding could help
    filter out impossible solution candidates that the model might
    produce if it only utilized the most recent observations on the neighbor,
    as in our previous work~\cite{CPR17}.
    %
	\begin{figure}\centering
		\includegraphics[width=1.\columnwidth]{fig_Concept}
		\caption{Illustration of our proposed pipeline in a snapshot example of
			$5$~robots at time $t$.
			Each robot has a limited view and a motion rule dependent on its neighbors
			except the \emph{Head} robot leading the team at the front.
			\emph{Tail} uses recent observations on its neighbor, \emph{Follower 3},
			which is denoted as $O(t-1,t)$. A sequence of historical poses, $h$, is
			also encoded for the model to make a final prediction on the
			unseen teammates.
		}
		\label{fig:Concept}
	\end{figure}
	%
	\begin{figure}\centering
		\includegraphics[width=1.\columnwidth]{fig_DL_Pipeline}
		\caption{Structure of our proposed deep neural network.
			This is an snapshot example when applied to a focused module of
			$3$~robots, called \emph{Tail, Follower, Head} within it, at time $t$.
			The Encoder-Decoder structure encodes
			1) historical positions and orientations of \emph{Follower} and \emph{Head}
			until $t-2$ and 2) the observed positions of the \emph{Follower} at $t-1$ and $t$.
			The decoder part learns to estimate 1) the position of \emph{Head} at $t-1$ and
			2) orientations of \emph{Follower} at $t-1$ and $t$ and \emph{Head} at $t-1$.
		}
		\label{fig:DL_Pipeline}
	\end{figure}
	%
    In addition to the more powerful deep-learning pipeline, we also
    make use of a more realistic robotic platform than in our previous
    work. Previously, we conducted all demonstrations on computer
    simulations, but in this work, we implement our approach in a
    realistic and reproducible physical environment realized by a widely
    available two-wheeled robotic platform, \emph{Thymio}~\cite{Shin14}.

%	All codes are available online\footnote{http://www.github.com/ctyeong}, and supplementary videos
%	are submitted as well.

    This paper is organized as follows. In
    Section~\ref{sec:related_work}, we explore related literature and
    the distinction of our work. Section~\ref{sec:rtl_problem} explains
    more details about our setting of RTL problem. Then, we introduce
    our \emph{IPY-Net} method in Section~\ref{sec:method}, and
    Section~\ref{sec:experiments} explains details about experiments
    performed on real robots, including data collection, hyperparameters
    used for learning, and the results. Lastly, we summarize our
    research and discuss future directions in
    Section~\ref{sec:discussion_and_future_work}.

	\section{Related work}
	\label{sec:related_work}

    In this section, we discuss results from the multi-robot
    systems literature similar to ours and elaborate on the distinct
    contribution of our work. We conclude this section with an
    elaboration of the differences between our results here and our
    previously presented work.

    \subsection{Cooperative localization and tracking}

    Although the RTL problem is superficially similar to other known
    cooperative localization and tracking problems~\cite{GKD04, LSRB16,
    FSDO10, CX14, DMG15}, such as cooperative SLAM, it is a distinct
    from general robotic localization. In RTL, the robot does not
    execute predictions on its own location but on its teammates using
    accessible information. Moreover, in contrast with cooperative
    localization approaches, robots in RTL are assumed to be
    communication free and thus not allowed to communicate with other
    members during the prediction of positions. Hence, in lieu of direct
    signalling, an underlying assumption of the RTL problem is that the
    robot behaviors are correlated with the state of the environment
    around them and thus contain cues about the state of their
    neighbors. In this sense, state observers in a networked robotic
    system are more similar to RTL than robotic
    localization~\cite{XNX10, GACM12}, but RTL does not depend upon
    knowledge of the structure of the underlying robotic controllers and
    does not require that robots move according to simplistic,
    analytically tractable dynamical models. Furthermore, we emphasize
    RTL solutions focused on scaling from training with small teams to
    implementation in potentially much larger and possibly variably
    sized teams.

	\subsection{Group state recognition}
	\label{sec:group_state_recognition}

    The RTL problem aims to infers the positional state of an entire
    multi-robot team using only locally obtainable information in the
    aspect of a robot member. This is because the knowledge about global
    configuration could help make a better decision for the sake of
    whole team. In this spirit, Brown and Goodrich~\cite{BG14} as well
    as Berger et al.~\cite{BSB16} show that local interactions of robots
    within a swarm can be used to classify swarm-level macroscopic
    structures, such as particular swarm-level shapes like \emph{flock}
    and \emph{torus}. In contrast, our work is to estimate the pose of
    robots themselves, which are the microscopic elements of the
    multi-robot team. Consequently, we make inferences on a space with
    far more degrees of freedom and require a more powerful regression
    model.

	\subsection{Behavioral cue interpretation}
	\label{sec:behavioral_cue_interpretation}

    In the RTL problem, the pose of remote robots is to be inferred from
    the motions of nearby robots performing otherwise nominal behaviors.
    This approach allows information to flow around a multi-robot team
    without traditional communication modalities for explicit
    signalling, such as radio communication. Motivated by similar
    constraints on reducing the use of these modalities, Novitzky at
    el.~\cite{NPCBW12} and Das et al.~\cite{DCV16} showed how robots
    performing a special behavior, similar to a ``waggle dance'' of
    honeybees~\cite{VonFrisch67}, could convey information visually or
    mechanically to remotely observing robots. Their approaches are
    different from ours in that we do not require robots to deviate from
    their normal behaviors for the purposes of explicit communication;
    we infer positions only from the latent information in nominal robot
    behavior and interactions.

	\subsection{Robot dynamics learning}
	\label{sec:robot_dynamics_learning}

    Byranvan and Fox~\cite{BF17} proposed a deep learning approach to
    predict the next visual frame given both a current visual frame as
    well as knowledge of a force acting on an object within the frame.
    One of the motivations for this work was to understand the dynamics
    of robotic arms and the relationship with control commands possibly
    executed. In the RTL problem described in Section~\ref{sec:intro},
    the \emph{Tail} robot may have accumulated a global shape of robot
    team over time, and it has to be able to predict the future
    formation as a new observation on its neighbor is provided, which
    could be viewed as gaining knowledge of an applied force on the
    team. Such a similarity inspired the architecture of our neural
    network model, but Byranvan and Fox focused on learning motions of
    rigid objects, while a chain of robots in our work can present much
    flexibility in team shape. In addition, the positional information
    about the nearest neighbor is only loosely analogous to the perfect
    knowledge of force used in the frame-prediction example.
    Consequently, our approach is a significant deviation from the one
    proposed by Byranvan and Fox~\cite{BF17}.

	\subsection{Contribution beyond past work}
	\label{sec:scalable_teammate_localization}

    We first presented the RTL problem in our previous
    work~\cite{CPR17}. To solve the problem with little robot-to-robot
    communication, the \emph{Tail} robot is designed to use a repetitive
    strategy of inference with which the predictions on a closer robot
    are used as input to prediction for more distant team members. Such
    an approach enables scaling the estimation capability from training
    on small teams to implementation in larger teams without further
    training. We use the same repetitive prediction scheme here;
    however, our focus is now on improving the regression engine to
    broaden its applicability from simulation to physically implemented
    robotic teams using commercially available, off-the-shelf robotic
    platforms.

	\section{RTL Problem \& System Design}
	\label{sec:rtl_problem}

    Here, we build on the problem and approach we introduced in our previous
    work~\cite{CPR17}. In particular, we consider a team of $n \in
    \{3,4,\dots\}$ robots designed to move in a line formation.
    Each robot maintains a programmed
    proximity with both the robot ahead of it and behind it, except for
    the \emph{Head} robot that leads the convoy and the \emph{Tail}
    that only follows its single neighbor ahead of it. Each robot in
    between \emph{Head} and \emph{Tail} are referred to as
    \emph{Follower}~$i$, where $i \in \{1, 2, \dots, n-2\}$ represents
    the position relative to the \emph{Tail} (i.e., \emph{Follower}~1 is
    closest to the \emph{Tail}).
%	For better readability, we
%	interchangeably denote the robots as \emph{H, T} and $F_{i}$.

    Every robot has the same sensory and motion capabilities and
    constraints, although they may take different actions according to
    motion rules depending on their roles. This leads to the
    \emph{Follower} and \emph{Tail} robots to behave differently based
    on the current positions of their close neighbors. For simplicity,
    we assume that every robot has an ability to accelerate fast enough
    to always keep the neighbors within their sensory range.

    The RTL is to localize all teammates from the view of \emph{Tail}
    only using locally observable information of the position position
    of \emph{Follower}~$1$ at every time step. We assume that until a
    specific time instant $\tau$, all the information about positions
    and orientations of all robots have been shared reliably with the
    \emph{Tail} robot, possibly via global communication, but continued
    information sharing is unavailable after time $\tau$ thus requiring
    \emph{Tail} to use this localization technique to extrapolate from
    the previously known reliable positions.

    Formally, at the time instant $\tau$, the following set of poses of
    all teammates is available:
    %
	\begin{equation}
		\{\vec{p}_{r@t},\theta_{r@t}\} \\
	    \label{eq:pose_set}
	\end{equation}
    %
    where $\vec{p}_{r@t} \overset{\Delta}{=}(x_{r@t}, y_{r@t})$ is the
    position of robot~$r$ at time $t$ where  $r \in \{T, F_{1}, F_{2},
    ..., F_{n-2}, H\}$ and $t \leq \tau$. The RTL problem is thus, at
    each time $t > \tau$, to observe the position of $F_1$,
    $\vec{p}_{F_{1@t}}$, and use all available information to predict
    the pose set in Eq.~\eqref{eq:pose_set} for time $t' > \tau$.

%	This may simulate some realistic scenarios where
%	an unexpected technical issue occurs at a time point disabling any robot-to-robot
%	data transfer, or the robot team intentionally stops the information
%	sharing when entering some specific regions either for security or for saving on the
%	communication cost.

	\section{Method}
	\label{sec:method}

%	In this section, we introduce more details about our method.
	First, we briefly revisit the scalable prediction approach, shown in~\cite{CPR17},
	to train the model on a small team and to be able to produce predictions
	even on a larger team without additional training.
	Then, we present the regressor built for learning not only with observations on
	the neighbor but also with a sequence of past team formations.

	\subsection{Scalable prediction}
	\label{sec:scalable_prediction}

	The basic idea in~\cite{CPR17} is that the \emph{Tail} uses all the available information
	such as the observation on its neighbor
	at $t$ and $t+1$ to predict $\vec{p}_{H@t}$ of \emph{Head} in its subteam of $3$~robots,
	which is actually \emph{Follower~2} in the entire team.
	After twice applications of such a prediction until $t+2$, the prediction outputs,
	$\vec{p}_{H@t}$ and $\vec{p}_{H@t+1}$, enable
	to perform the same process in the subteam of the next $3$~robots, which are
	\emph{Follower~1, Follower~2}, and \emph{Follower~3}, as if \emph{Follower~1} and
	\emph{Follower~3} were \emph{Tail} and \emph{Head} in that subteam, respectively.

	Since the inference is applied in each of subteam in a repetitive manner,
	a predictor can be trained simply with a $3$-robot team and still be used for a larger
	size of team without additional training for it. However, the extension of
	modular prediction brings about a time delay in making a prediction for a far robot.
	Specifically, a prediction for \emph{Head} in a subteam requires a series of at least
	$2$~observations on \emph{Follower}, and in other words,
	the initial position of robot~$i$ can be estimated at time $i$ for the first time
	where robot~$i$ refers to the robot that is $i$ robots ahead of \emph{Tail}.

	In the next section, we discuss how to incorporate such a prediction strategy with
	the proposed model regressor in more technical details.


	\subsection{Regression model}
	\label{sec:regression_model}

	Because the learned regressor is designed to work in a modular team of $3$ robots,
	all the notations here for \emph{Head, Follower}, and \emph{Tail} only indicate
	them. In addition, all positions noted here are assumed to be relative positions
	to the \emph{Tail} robot, since in practice, the \emph{Tail} has to understand
	positions of others by projection onto the local coordinate system centered
	at itself. Though a similar conversion may be considered for orientation,
	we use absolute direction in this work because assuming robots equipped
	with an instrument such as a electric compass is acceptable.

	Previously, in~\cite{CPR17} we used a series of fully-connected~(FC) layers
	to only take the observation input and estimate the position and orientation
	of interest:

	\begin{equation}
	Y = f(X_{obs})
	\end{equation}

	\begin{flushleft}
	where $f$ is a series of \emph{FC} layers,
	$X_{obs} = (\vec{p}_{F@t}, \theta_{F@t}, \vec{p}_{F@t+1})$, and
	$Y = (\hat{\vec{p}}_{H@t}, \hat{\theta}_{F@t+1})$
	\end{flushleft}

	As shown in Fig.~\ref{fig:DL_Pipeline}, however, our proposed model uses not only
	an observation input but also a historical sequence of poses. First of all, we
	use a slightly different observation encoding:

	\begin{equation}
	o = f_{obs}(X_{obs})
	\end{equation}

	where $f_{obs}$ is a FC layer,
		$X_{obs} = (\vec{p}_{F@t}, \vec{p}_{F@t+1})$,
		and $o \in \mathbb{R}^{k}$ is the encoded observation feature
		where $k$ is the size of FC layer.

	A historical sequence of poses is encoded by a bi-directional LSTM layer~\cite{Wu16}:

	\begin{equation}
	h = f_{hist}(X_{hist})
	\end{equation}


     where $f_{hist}$ is a LSTM layer,
		$X_{hist} = (\vec{p}_{H@t-l+1:t}, \theta_{H@t-l+1:t},
		             \vec{p}_{F@t-l+1:t}, \theta_{F@t-l+1:t})$,
		and $h \in \mathbb{R}^{2 \times m}$ is the encoded history feature
	where $l$ is the length of historical sequence,
	and $m$ is the size of LSTM layer.

	Then, $o$ and $h$ are synthesized by a layer $\phi$, which is
	passed as input to two separate final regressors, $g_{p}$ and $g_\theta$.

	\begin{equation}
	\begin{split}
	Y_{p} = g_{p}(\phi(o, h)),\\
	Y_{\theta}= g_{\theta}(\phi(o, h))
	\end{split}
	\label{eq:regression_output}
	\end{equation}

	where
	$Y_{p} = \hat{\vec{p}}_{H@t}$ and
	$Y_{\theta} = (\hat{\theta}_{F@t}, \hat{\theta}_{H@t}, \hat{\theta}_{F@t+1})$.

    For fusion of $o$ and $h$ features, layer $\phi$ in Eq.~\eqref{eq:regression_output}
	could be implemented by any type of layer. During our experiments, we
	built a FC layer of size $d$ to find a nonlinear relationship between the input
	features and achieved a satisfactory performance.

	Furthermore, $\hat{\theta}_{F@t+1}$ gained with $\hat{\theta}_{F@t}$ is
	estimated again when $\hat{\theta}_{F@t+2}$ is estimated at the next
	prediction step. Although our model keeps the later estimate only,
	we discovered that involving it in both steps can regulate the regressor
	during learning to produce a model that achieves a better score in validation.

	To find a best combination of model parameters mentioned above, we performed
	an extensive random search with choices of other learning parameters and
	finally set $k=80, m=160$, and $d=160$.


	\section{Experiments}
	\label{sec:experiments}

	\setlength{\tabcolsep}{0.5em} % for the horizontal padding
	{\renewcommand{\arraystretch}{1.2}% for the vertical padding
		\begin{table}[t]
			\centering
			\begin{tabular}{|c|c|c|c|c|}
				\hline
							&  Duration & Num. of Samples & Num. of Instances  \\ \hline
				$3$ robots & $100.6$ minutes & $6,975$ & $465$  \\ \hline
				$5$ robots & $45.0$ minutes  & $8,736$ & $208$  \\ \hline
			\end{tabular}
			\caption{Description of data collected from executions of $3$-robot and $5$-robot teams.}
			\label{table:data_description}
		\end{table}
	}

	To demonstrate the effectiveness of our method, we employ a physical robotic platform,
	\emph{Thymio}~\cite{Shin14}, which allows to execute a team of small two-wheeled
	mobile robots. We use a central computer connected with a overhead camera to simulate
	better proximity
	sensors, a more powerful computing power, and a GPS system that would easily run on each robot in
	real scenario.
	Specifically, the central system is set up to detect the locations of robots in real
	time using off-the-shelf computer vision packages
	and communicate with a \emph{Raspberry Pi} board~\cite{Upton14} on each robot to send
	the next command relying on its neighbors.
	The command was essentially obtained based on the formulation in~\cite{CPR17}, but
    as the robot was asked to move backward, we just set it not to move for its
	neighbor behind to catch it up, which helped gain smoother trajectories of the team
	and eventually avoid possible collisions between robots.

	Although the experiment setting involves
	some external computations and sensors due to limited capability of \emph{Thymio},
	most of realistic assumptions still hold from the physical constraints and disturbance
	during execution. For better understanding of readers on the prepared environment,
	we submit a supplementary video.

	Finally, we collected the pose data from two robot teams, one of $3$~robots and one
	of $5$~robots, that ran separately in an arena of $2.5 m \times 1.9 m$.
	The location detection was performed at $4$ frames per second at each of
	which a new command was received by each robot. Also, all pose data was
	collected at the rate of $2$ frames per second,
	which was not necessarily synchronized with the command timing.
	We set the length of history to $5$~seconds~($10$~time steps in data recording) and
	the time window for prediction to the next $8$~seconds~($16$~time steps).
	Table~\ref{table:data_description} provides details about the collected data
	where a sample refers to a set of coordinates and orientations in a subteam
	with which a prediction can be performed, and an instance is set of all available samples for $13$ seconds from the entire team. We clustered recordings so that
	an instance has at least $7$-second time gap to another ensuring
	the motions between separate instances have little dependency.

	$60$\% data from the $3$-robot team is used to train our model, and another $10\%$
	was set aside for validation. At each epoch of training, a validation followed so that
	the learned weights that achieved the best validation performance were saved.
	The rest of $20\%$ data and all the data from $5$-robot were used to test the model.

	Our model is implemented in \emph{Tensorflow} \emph{Python} library \footnote{https://www.tensorflow.org/}
	to realize the entire pipeline, and it was trained to minimize
	loss functions such as Euclidean distance and mean absolute errors
	for position and orientation estimation, respectively.

	We compare our proposed model to two different approaches such as:
	\begin{itemize}
		\item \emph{$2$X Heuristic}:
		The prediction on \emph{Head} within a modular subteam is performed by doubling the vector
		$\vec{p}_{F} - \vec{p}_{T}$.

		\item \emph{FC}:
		Two fully connected layers run in the predictor without historical
		information, which is based on~\cite{CPR17}.

	\end{itemize}

%	\subsection{Data collection}
%	\label{sec:data_collection}


%	\subsection{Results}
%	\label{sec:results}

	\begin{figure}[t]
	\centering
	\includegraphics[width=1.\columnwidth]{fig_macro_eval}
	\caption{Average accumulated error of each model in two different sizes of robot team.
		For each of machine learning methods, the mean performance of $5$~separate training
		sessions is reported with a error bar to visualize the performance variation.
	}
	\label{fig:macro_eval}
	\end{figure}


	\begin{figure}[t]
		\centering
		\includegraphics[width=1.\columnwidth]{fig_micro_eval}
		\caption{Average step-wise error for different target robots in $5$-robot team.
			For each model, all the prediction errors at different time steps are averaged
			for a specific robot. The error bars represent the standard deviation of
			$5$~separate models in terms of the performance metric. For the sake of
			visualization, the error for \emph{Follower~3} is omitted, but note that
			in any model, the error ranges between the two visualized errors at every step.
		}
		\label{fig:micro_eval}
	\end{figure}


	\subsection{Overall performance}
	\label{sec:overall_performance}

	First of all, we evaluate each model in a macroscopic view, as shown in
	Fig.~\ref{fig:macro_eval}, where each model is tested with the
	test data of $3$-robot team and $5$-robot team separately, and in each case,
	the averaged error for all position predictions is reported.
	Specifically, in the case of $3$~robots, the average error is calculated only on
	the prediction for the \emph{Head} robot, while in the $5$-robot case,
	it involves all predictions for \emph{Follower 2}, \emph{Follower 3}, and \emph{Head}.
	Moreover, for every model except the \emph{2X Heuristic}, the mean performance of
	$5$~separate learning sessions is reported with the standard deviation.

	Figure~\ref{fig:macro_eval} displays that the machine learning methods outperform
	the \emph{2X Heuristic} in any case. In particular, the performance gap between
	\emph{2X Heuristic} and \emph{FC} becomes much larger in $5$-robot case suggesting that
	during the data collection, we added much randomness to the shape of the robot team.

	Also, our model clearly shows the performance improvement over the \emph{FC} model, since
	the average error was reduced by $47\%$ and $40\%$ in the $3$-robot and $5$-robot case,
	respectively. In addition, considering the diameter of a \emph{Thymio} robot is
	$12$~cm, as only three robots are deployed, the average distance between the prediction
	and the true position is shorter than the length of the body. This overall result proves
	the effectiveness of encoding a sequence of historical behaviors as a feature input
	over the model fed only with very recent observation.


	\subsection{Microscopic analysis}
	\label{sec:microscopic_analysis}

	In this section, we analyze the performance of the machine learning based models
	in more fine-grained perspective. For each model, Fig.~\ref{fig:micro_eval} visualizes the
	step-wise error for different target robots~(\emph{Follower 2} and \emph{Head})
	in $5$-robot case, which is calculated by averaging the errors of each step across instances.
	Such a way of evaluation can offer us a crucial insight to understand an approximate time frame
	within which a level of average error could be ensured for a specific robot. During this
	evaluation, we also had $5$~separate training sessions to gain the mean and the variability
	of the performance.

	Figure~\ref{fig:micro_eval} shows that for any target robot, the error increases over time due to the
	design of the repetitive prediction method where previous errors would negatively
	impact the prediction power for the following steps. In a similar sense, each model
	brings about larger error on the \emph{Head} prediction than on the \emph{Follower 2}.

	At every time step, our approach causes less error than the \emph{FC} model for any robot, and
    the visualized error bars confirm that the performance gap is significant in any case.
    Especially, for first $4$ seconds, the prediction on \emph{Head} from our model appears
    more accurate than the prediction on \emph{Follower~2} from the \emph{FC} implying that
    until then, lower errors is produced on \emph{Follower~3} as well.

    \subsubsection{Limitation}
	\label{sec:limitation}

	Figure~\ref{fig:micro_eval} shows that using our regressor, we could trust the inference
	outcome until before first $3$~seconds and $4.5$~seconds depending on which robot to target between
	\emph{Head} and \emph{Follower~2}, since the body of a \emph{Thymio} robot is around
	$12$~cm. This actually relies on the acceptable degree of error and the diameter of
	robotic platform used in the problem to tackle. Still, our result demonstrates the potential
	of effectively alleviating prediction error by adopting a well-designed model, and also,
	the relatively short time period of guaranteed localization may be sufficient to
	save on the communication bandwidth within the robot team.

	Also, the error distance when our model performs for \emph{Head} increases relatively slow
	initially but after some point, the increase rate gradually becomes higher. This is contrast to the
	case of \emph{FC}, which leads to a constant increase at each time step.
	This may be because as the historical features are extracted more from previous predictions
	instead of the truths, it could cause a more accumulated error in prediction outcome than
	using previous predictions only as observation input as in \emph{FC}.

	\begin{figure*}[t]
		\centering
		\includegraphics[width=2.\columnwidth]{fig_preds}
		\caption{Sample video frames with prediction results of our proposed model
			for three different instances of $5$~robot team.
			Each row presents four frames of an instance for first $4$~seconds.
			The yellow robot is \emph{Tail}, the pink is \emph{Head}, and
			in-between are \emph{Follower} robots.
			The colored circles are predicted position outputs, in each
			of which a black arrow is drawn to indicate the predicted orientation as well.
			The colored triangles with a black line through the center are the
			results of localization detection used in data collection stage.
			A small red dot on the arena in each frame is the random destination the
			\emph{Head} is moving toward, which is resampled at intervals.
		}
		\label{fig:preds}
	\end{figure*}


	\subsection{Qualitative results}
	\label{sec:qualitative_results}

	Figure~\ref{fig:preds} displays three different instances of $5$-robot team
	in which both the true positions of all robots and the predictions on them are represented.
	In overall, the predictions
	until $4$~seconds are shown to be reliable, although there are some errors while the
	group of robots is turning with a high angle and the \emph{Head} changes its destination.
	An observation is that the predictions tend to be located more inward the curve the team is moving on.
	Yet, even when the prediction for a nearer robot is away from the truth, the prediction
	for a farther robot appears correctly in some cases.
	Such a correction occurs probably because even if an observation input from a nearer robot was
	actually inaccurate, the past positions in input could alleviate the impact of it and still
	help generate a decent estimation result.


	\section{Discussion \& Future work}
	\label{sec:discussion_and_future_work}

	We have proposed a new design of regression model that can take into consideration
	the historical behavioral sequences to solve the RTL problem on a real robotic platform.
	Following the fundamental of scalable localization algorithm introduced in~\cite{CPR17},
	our approach enables a robot at one end of string of robots to make repetitive predictions
	on poses of unseen teammates by taking prediction outcomes for nearer robots as input to
	prediction for farther ones.

	We utilized a physical robotic system, \emph{Thymio}, to collect datasets from
	$3$-robot and $5$-robot teams. Through empirical experiments, we explained that in overall,
	the proposed machine learning model offers more accurate estimation than other baselines.
	In addition, our analysis on timestep-wise errors helped
	explore the benefits from encoding historical behavior sequences as well as a
	drawback of it. Lastly, we visualized prediction outcomes from a set of samples to
	illustrate specific cases where the features from past behaviors could also reduce the
	estimation error passed from previous predictions for nearer robots.

	In the future work, we could more deeply investigate the factors in model accuracy such as
	the length of history or types of team behavior that might favor the prediction scheme.
	Also, since the LSTM layer is exposed
	to various evolutions of team shape during training, the vector representation of it
	may be examined to characterize team states and finally detect abnormality of the whole
	system.


{\small
	\bibliographystyle{IEEEtran}
	%\bibliography{IEEEabrv, IEEEexample}
	\bibliography{IEEEexample}
}


\end{document}
